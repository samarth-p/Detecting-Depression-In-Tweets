{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Depression Detection in Social Media Posts\n",
    "\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\samarth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\samarth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import ftfy\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from math import exp\n",
    "from numpy import sign\n",
    "\n",
    "from sklearn.metrics import  classification_report, confusion_matrix, accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "np.random.seed(1234)\n",
    "\n",
    "DEPRES_NROWS = 100000  # number of rows to read from DEPRESSIVE_TWEETS_CSV\n",
    "RANDOM_NROWS = 100000  # number of rows to read from RANDOM_TWEETS_CSV\n",
    "MAX_SEQUENCE_LENGTH = 140 # Max tweet size\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "TRAIN_SPLIT = 0.6\n",
    "TEST_SPLIT = 0.2\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Data\n",
    "\n",
    "Loading depressive tweets scraped from twitter using TWINT and random tweets from Kaggle dataset twitter_sentiment.\n",
    "\n",
    "#### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPRESSIVE_TWEETS_CSV = 'Datasets/depression_2019.csv'\n",
    "RANDOM_TWEETS_CSV = 'Datasets/Sentiment Analysis Dataset 2.csv'\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressive_tweets_df = pd.read_csv(DEPRESSIVE_TWEETS_CSV, usecols = range(10,11), nrows = DEPRES_NROWS)\n",
    "random_tweets_df = pd.read_csv(RANDOM_TWEETS_CSV, encoding = \"ISO-8859-1\", usecols = range(0,4), nrows = RANDOM_NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4/5 than that what's most important to me is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kidding Iâ€™m lonely boys hate me if yâ€™all know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thanks to all of you who have have been such a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iâ€™ll be lonely tonight ðŸ™„</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And pops goes \"I miss having you guys around t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  4/5 than that what's most important to me is t...\n",
       "1  kidding Iâ€™m lonely boys hate me if yâ€™all know ...\n",
       "2  Thanks to all of you who have have been such a...\n",
       "3                           Iâ€™ll be lonely tonight ðŸ™„\n",
       "4  And pops goes \"I miss having you guys around t..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressive_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ã¯Â»Â¿ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ã¯Â»Â¿ItemID  Sentiment SentimentSource  \\\n",
       "0          1          0    Sentiment140   \n",
       "1          2          0    Sentiment140   \n",
       "2          3          1    Sentiment140   \n",
       "3          4          0    Sentiment140   \n",
       "4          5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Word2Vec Model\n",
    "\n",
    "Load the pretrained vectors for the Word2Vec model. Using a Keyed Vectors file, we can get the embedding of any word by calling `.word_vec(word)` and we can get all the words in the model's vocabulary through `.vocab`. Download the Word2Vec model [here](https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Preprocessing the tweets in order to:\n",
    "* Remove links and images\n",
    "* Remove hashtags\n",
    "* Remove @ mentions\n",
    "* Remove emojis\n",
    "* Remove stop words\n",
    "* Remove punctuation\n",
    "* Get rid of stuff like \"what's\" and making it \"what is'\n",
    "* Stem words so they are all the same tense (e.g. ran -> run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand Contraction\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    cleaned_tweets = []\n",
    "    for tweet in tweets:\n",
    "        tweet = str(tweet)\n",
    "        # if url links then dont append to avoid news articles\n",
    "        # also check tweet length, save those > 10 (length of word \"depression\")\n",
    "        if re.match(\"(\\w+:\\/\\/\\S+)\", tweet) == None and len(tweet) > 10:\n",
    "            #remove hashtag, @mention, emoji and image URLs\n",
    "            tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(\\#[A-Za-z0-9]+)|(<Emoji:.*>)|(pic\\.twitter\\.com\\/.*)\", \" \", tweet).split())\n",
    "            \n",
    "            #fix weirdly encoded texts\n",
    "            tweet = ftfy.fix_text(tweet)\n",
    "            \n",
    "            #expand contraction\n",
    "            tweet = expandContractions(tweet)\n",
    "\n",
    "            #remove punctuation\n",
    "            tweet = ' '.join(re.sub(\"([^0-9A-Za-z \\t])\", \" \", tweet).split())\n",
    "\n",
    "            #stop words\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            word_tokens = nltk.word_tokenize(tweet) \n",
    "            filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "            tweet = ' '.join(filtered_sentence)\n",
    "\n",
    "            #stemming words\n",
    "            tweet = PorterStemmer().stem(tweet)\n",
    "            \n",
    "            cleaned_tweets.append(tweet)\n",
    "\n",
    "    return cleaned_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the preprocessing `clean_text` function to every element in the depressive tweets and random tweets data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depressive_tweets_arr = [x for x in depressive_tweets_df[5]]\n",
    "depressive_tweets_arr = [x for x in depressive_tweets_df[\"tweet\"]]\n",
    "random_tweets_arr = [x for x in random_tweets_df['SentimentText']]\n",
    "X_d = clean_tweets(depressive_tweets_arr)\n",
    "X_r = clean_tweets(random_tweets_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Using a Tokenizer to assign indices and filtering out unfrequent words. Tokenizer creates a map of every unique word and an assigned index to it. The parameter called num_words indicates that we only care about the top 20000 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X_d + X_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('Tokenizers/tokenizer_keywords.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the tokenizer to depressive tweets and random tweets data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_d = tokenizer.texts_to_sequences(X_d)\n",
    "sequences_r = tokenizer.texts_to_sequences(X_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique words in tokenizer. Has to be <= 20,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124011 unique tokens\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad sequences all to the same length of 140 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_d tensor: (98620, 140)\n",
      "Shape of data_r tensor: (99832, 140)\n"
     ]
    }
   ],
   "source": [
    "data_d = pad_sequences(sequences_d, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_r = pad_sequences(sequences_r, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data_d tensor:', data_d.shape)\n",
    "print('Shape of data_r tensor:', data_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Matrix\n",
    "\n",
    "The embedding matrix is a `n x m` matrix where `n` is the number of words and `m` is the dimension of the embedding. In this case, `m=300` and `n=20000`. We take the min between the number of unique words in our tokenizer and max words in case there are less unique words than the max we specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for (word, idx) in word_index.items():\n",
    "    if word in word2vec.vocab and idx < MAX_NB_WORDS:\n",
    "        embedding_matrix[idx] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and Formatting Data\n",
    "\n",
    "Assigning labels to the depressive tweets and random tweets data, and splitting the arrays into training (60%), validation (20%), and test data (20%). Combine depressive tweets and random tweets arrays and shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning labels to the depressive tweets and random tweets data\n",
    "labels_d = np.array([1] * DEPRES_NROWS)\n",
    "labels_r = np.array([0] * RANDOM_NROWS)\n",
    "\n",
    "# Splitting the arrays into test (60%), validation (20%), and train data (20%)\n",
    "perm_d = np.random.permutation(len(data_d))\n",
    "idx_train_d = perm_d[:int(len(data_d)*(TRAIN_SPLIT))]\n",
    "idx_test_d = perm_d[int(len(data_d)*(TRAIN_SPLIT)):int(len(data_d)*(TRAIN_SPLIT+TEST_SPLIT))]\n",
    "idx_val_d = perm_d[int(len(data_d)*(TRAIN_SPLIT+TEST_SPLIT)):]\n",
    "\n",
    "perm_r = np.random.permutation(len(data_r))\n",
    "idx_train_r = perm_r[:int(len(data_r)*(TRAIN_SPLIT))]\n",
    "idx_test_r = perm_r[int(len(data_r)*(TRAIN_SPLIT)):int(len(data_r)*(TRAIN_SPLIT+TEST_SPLIT))]\n",
    "idx_val_r = perm_r[int(len(data_r)*(TRAIN_SPLIT+TEST_SPLIT)):]\n",
    "\n",
    "# Combine depressive tweets and random tweets arrays\n",
    "data_train = np.concatenate((data_d[idx_train_d], data_r[idx_train_r]))\n",
    "labels_train = np.concatenate((labels_d[idx_train_d], labels_r[idx_train_r]))\n",
    "data_test = np.concatenate((data_d[idx_test_d], data_r[idx_test_r]))\n",
    "labels_test = np.concatenate((labels_d[idx_test_d], labels_r[idx_test_r]))\n",
    "data_val = np.concatenate((data_d[idx_val_d], data_r[idx_val_r]))\n",
    "labels_val = np.concatenate((labels_d[idx_val_d], labels_r[idx_val_r]))\n",
    "\n",
    "# Shuffling\n",
    "perm_train = np.random.permutation(len(data_train))\n",
    "data_train = data_train[perm_train]\n",
    "labels_train = labels_train[perm_train]\n",
    "perm_test = np.random.permutation(len(data_test))\n",
    "data_test = data_test[perm_test]\n",
    "labels_test = labels_test[perm_test]\n",
    "perm_val = np.random.permutation(len(data_val))\n",
    "data_val = data_val[perm_val]\n",
    "labels_val = labels_val[perm_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model (LSTM + CNN)\n",
    "\n",
    "The model takes in an input and then outputs a single number representing the probability that the tweet indicates depression. The model takes in each input sentence, replace it with it's embeddings, then run the new embedding vector through a convolutional layer. CNNs are excellent at learning spatial structure from data, the convolutional layer takes advantage of that and learn some structure from the sequential data then pass into a standard LSTM layer. Last but not least, the output of the LSTM layer is fed into a standard Dense model for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\samarth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\samarth\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Embedded layer\n",
    "model.add(Embedding(len(embedding_matrix), EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "# Convolutional Layer\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "# LSTM Layer\n",
    "model.add(LSTM(300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 140, 300)          6000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 140, 32)           28832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 70, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 70, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300)               399600    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 6,428,733\n",
      "Trainable params: 428,733\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Training the Model\n",
    "\n",
    "The model is trained `EPOCHS` time, and Early Stopping argument is used to end training if the loss or accuracy don't improve within 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\samarth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 119071 samples, validate on 39691 samples\n",
      "Epoch 1/10\n",
      "119071/119071 [==============================] - 429s 4ms/step - loss: 0.1406 - acc: 0.9529 - val_loss: 0.1107 - val_acc: 0.9650\n",
      "Epoch 2/10\n",
      "119071/119071 [==============================] - 428s 4ms/step - loss: 0.1077 - acc: 0.9657 - val_loss: 0.1049 - val_acc: 0.9669\n",
      "Epoch 3/10\n",
      "119071/119071 [==============================] - 418s 4ms/step - loss: 0.0991 - acc: 0.9680 - val_loss: 0.1031 - val_acc: 0.9675\n",
      "Epoch 4/10\n",
      "119071/119071 [==============================] - 417s 4ms/step - loss: 0.0935 - acc: 0.9704 - val_loss: 0.1022 - val_acc: 0.9686\n",
      "Epoch 5/10\n",
      "119071/119071 [==============================] - 417s 4ms/step - loss: 0.0876 - acc: 0.9724 - val_loss: 0.1032 - val_acc: 0.9677\n",
      "Epoch 6/10\n",
      "119071/119071 [==============================] - 416s 3ms/step - loss: 0.0839 - acc: 0.9737 - val_loss: 0.1039 - val_acc: 0.9670\n",
      "Epoch 7/10\n",
      "119071/119071 [==============================] - 417s 4ms/step - loss: 0.0796 - acc: 0.9752 - val_loss: 0.1029 - val_acc: 0.9674\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "hist = model.fit(data_train, labels_train, \\\n",
    "        validation_data=(data_val, labels_val), \\\n",
    "        epochs=EPOCHS, batch_size=40, shuffle=True, \\\n",
    "        callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/depression_keywords_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize history for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "labels_pred = model.predict(data_test)\n",
    "labels_pred = np.round(labels_pred.flatten())\n",
    "accuracy = accuracy_score(labels_test, labels_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1, precision, and recall scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     19966\n",
      "           1       0.99      0.95      0.97     19724\n",
      "\n",
      "    accuracy                           0.97     39690\n",
      "   macro avg       0.97      0.97      0.97     39690\n",
      "weighted avg       0.97      0.97      0.97     39690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_test, labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_example(tweet):\n",
    "    tweet_list = []\n",
    "    tweet_list.append(tweet)\n",
    "    cleaned_list = clean_tweets(tweet_list)\n",
    "    sequence = tokenizer.texts_to_sequences(cleaned_list)\n",
    "    data = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    prediction = model.predict_classes(data)\n",
    "    if prediction[0][0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "tweet = \"Depression is eating me up\"\n",
    "predict_example(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Comparing the Model to Base Line\n",
    "\n",
    "In order to evaluate the effectiveness of the LSTM + CNN model, a logistic regression model is trained with the same train data and the same number of epochs, and tested with the same test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression Base Line Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    \"\"\"\n",
    "    Class to represent a logistic regression model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, l_rate, epochs, n_features):\n",
    "        \"\"\"\n",
    "        Create a new model with certain parameters.\n",
    "\n",
    "        :param l_rate: Initial learning rate for model.\n",
    "        :param epoch: Number of epochs to train for.\n",
    "        :param n_features: Number of features.\n",
    "        \"\"\"\n",
    "        self.l_rate = l_rate\n",
    "        self.epochs = epochs\n",
    "        self.coef = [0.0] * n_features\n",
    "        self.bias = 0.0\n",
    "\n",
    "    def sigmoid(self, score, threshold=20.0):\n",
    "        \"\"\"\n",
    "        Prevent overflow of exp by capping activation at 20.\n",
    "\n",
    "        :param score: A real valued number to convert into a number between 0 and 1\n",
    "        \"\"\"\n",
    "        if abs(score) > threshold:\n",
    "            score = threshold * sign(score)\n",
    "        activation = exp(score)\n",
    "        return activation / (1.0 + activation)\n",
    "\n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Given an example's features and the coefficients, predicts the class.\n",
    "\n",
    "        :param features: List of real valued features for a single training example.\n",
    "\n",
    "        :return: Returns the predicted class (either 0 or 1).\n",
    "        \"\"\"\n",
    "        value = sum([features[i]*self.coef[i] for i in range(len(features))]) + self.bias\n",
    "        return self.sigmoid(value)\n",
    "\n",
    "    def sg_update(self, features, label):\n",
    "        \"\"\"\n",
    "        Computes the update to the weights based on a predicted example.\n",
    "\n",
    "        :param features: Features to train on.\n",
    "        :param label: Corresponding label for features.\n",
    "        \"\"\"\n",
    "        yhat = self.predict(features)\n",
    "        e = label - yhat\n",
    "        self.bias = self.bias + self.l_rate * e * yhat * (1-yhat)\n",
    "        for i in range(len(features)):\n",
    "            self.coef[i] = self.coef[i] + self.l_rate * e * yhat * (1-yhat) * features[i]\n",
    "        return\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes logistic regression coefficients using stochastic gradient descent.\n",
    "\n",
    "        :param X: Features to train on.\n",
    "        :param y: Corresponding label for each set of features.\n",
    "\n",
    "        :return: Returns a list of model weight coefficients where coef[0] is the bias.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            for features, label in zip(X, y):\n",
    "                self.sg_update(features, label)\n",
    "        return self.bias, self.coef\n",
    "\n",
    "def get_accuracy(y_bar, y_pred):\n",
    "    \"\"\"\n",
    "    Computes what percent of the total testing data the model classified correctly.\n",
    "\n",
    "    :param y_bar: List of ground truth classes for each example.\n",
    "    :param y_pred: List of model predicted class for each example.\n",
    "\n",
    "    :return: Returns a real number between 0 and 1 for the model accuracy.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for i in range(len(y_bar)):\n",
    "        if y_bar[i] == y_pred[i]:\n",
    "            correct += 1\n",
    "    accuracy = (correct / len(y_bar)) * 100.0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogReg(LEARNING_RATE, EPOCHS, len(data_train[0]))\n",
    "bias_logreg, weights_logreg = logreg.train(data_train, labels_train)\n",
    "y_logistic = [round(logreg.predict(example)) for example in data_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the accuracy of the logistic regression model predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 49.836\n"
     ]
    }
   ],
   "source": [
    "accuracy_logistic = get_accuracy(y_logistic, labels_test)\n",
    "print('Logistic Regression Accuracy: {:0.3f}'.format(accuracy_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train\n",
    "y_train = labels_train\n",
    "X_test = data_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting Decision Tree Classification to the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6896021768159029\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15047  4920]\n",
      " [ 7400 12324]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.75      0.71     19967\n",
      "           1       0.71      0.62      0.67     19724\n",
      "\n",
      "    accuracy                           0.69     39691\n",
      "   macro avg       0.69      0.69      0.69     39691\n",
      "weighted avg       0.69      0.69      0.69     39691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train\n",
    "y_train = labels_train\n",
    "X_test = data_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting Decision Tree Classification to the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8693877551020408\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17355  2611]\n",
      " [ 2573 17151]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87     19966\n",
      "           1       0.87      0.87      0.87     19724\n",
      "\n",
      "    accuracy                           0.87     39690\n",
      "   macro avg       0.87      0.87      0.87     39690\n",
      "weighted avg       0.87      0.87      0.87     39690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train\n",
    "y_train = labels_train\n",
    "X_test = data_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting SVM to the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.681708238851096\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17942  2024]\n",
      " [10609  9115]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.90      0.74     19966\n",
      "           1       0.82      0.46      0.59     19724\n",
      "\n",
      "    accuracy                           0.68     39690\n",
      "   macro avg       0.72      0.68      0.67     39690\n",
      "weighted avg       0.72      0.68      0.67     39690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
